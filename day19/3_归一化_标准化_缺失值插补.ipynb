{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始数据：\n",
      " [[  1.   2.   3.]\n",
      " [  4.   5.   6.]\n",
      " [  7.   8.   9.]\n",
      " [ 10.  11.  12.]\n",
      " [ 13.  14.  15.]\n",
      " [ 16.  17.  18.]\n",
      " [-19. -20.  21.]\n",
      " [221. 231. 214.]]\n",
      "\n",
      "归一化后的数据：\n",
      " [[0.08333333 0.0876494  0.        ]\n",
      " [0.09583333 0.09960159 0.01421801]\n",
      " [0.10833333 0.11155378 0.02843602]\n",
      " [0.12083333 0.12350598 0.04265403]\n",
      " [0.13333333 0.13545817 0.05687204]\n",
      " [0.14583333 0.14741036 0.07109005]\n",
      " [0.         0.         0.08530806]\n",
      " [1.         1.         1.        ]]\n",
      "\n",
      "归一化到[-1,1]后的数据：\n",
      " [[-0.83333333 -0.8247012  -1.        ]\n",
      " [-0.80833333 -0.80079681 -0.97156398]\n",
      " [-0.78333333 -0.77689243 -0.94312796]\n",
      " [-0.75833333 -0.75298805 -0.91469194]\n",
      " [-0.73333333 -0.72908367 -0.88625592]\n",
      " [-0.70833333 -0.70517928 -0.85781991]\n",
      " [-1.         -1.         -0.82938389]\n",
      " [ 1.          1.          1.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "# 创建更多数据\n",
    "datas = np.array([\n",
    "    [1.0, 2.0, 3.0],\n",
    "    [4.0, 5.0, 6.0],\n",
    "    [7.0, 8.0, 9.0],\n",
    "    [10.0, 11.0, 12.0],\n",
    "    [13.0, 14.0, 15.0],\n",
    "    [16.0, 17.0, 18.0],\n",
    "    [-19.0, -20.0, 21.0],\n",
    "    [221.0, 231.0, 214.0]\n",
    "])\n",
    "\n",
    "# 创建MinMaxScaler对象\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# 拟合并转换数据\n",
    "scaled_data = scaler.fit_transform(datas)\n",
    "\n",
    "print(\"原始数据：\\n\", datas)\n",
    "print(\"\\n归一化后的数据：\\n\", scaled_data)\n",
    "# 创建MinMaxScaler对象，设置归一化范围为[-1, 1]\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))#用feature_range参数设置归一化范围\n",
    "\n",
    "# 拟合并转换数据\n",
    "scaled_data = scaler.fit_transform(datas)\n",
    "\n",
    "print(\"\\n归一化到[-1,1]后的数据：\\n\", scaled_data)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "包含异常点的原始数据：\n",
      " [[   1.    2.    3.]\n",
      " [   4.    5.    6.]\n",
      " [   7.    8.    9.]\n",
      " [  10.   11.   12.]\n",
      " [ 100.  200.  300.]\n",
      " [ -50. -100. -150.]\n",
      " [  22.   23.   24.]\n",
      " [  25.   26.   27.]]\n",
      "\n",
      "标准化后的数据：\n",
      " [[-0.35817821 -0.25768568 -0.22331267]\n",
      " [-0.28073427 -0.21878973 -0.19742134]\n",
      " [-0.20329034 -0.17989378 -0.17153002]\n",
      " [-0.1258464  -0.14099783 -0.14563869]\n",
      " [ 2.19747173  2.30944714  2.33992836]\n",
      " [-1.67472515 -1.58014804 -1.54377016]\n",
      " [ 0.18392935  0.01458598 -0.0420734 ]\n",
      " [ 0.26137329  0.05348193 -0.01618208]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 创建包含异常点的数据\n",
    "data_with_outliers = np.array([\n",
    "    [1.0, 2.0, 3.0],\n",
    "    [4.0, 5.0, 6.0],\n",
    "    [7.0, 8.0, 9.0],\n",
    "    [10.0, 11.0, 12.0],\n",
    "    [100.0, 200.0, 300.0],  # 异常点\n",
    "    [-50.0, -100.0, -150.0],  # 异常点\n",
    "    [22.0, 23.0, 24.0],\n",
    "    [25.0, 26.0, 27.0]\n",
    "])\n",
    "\n",
    "# 创建StandardScaler对象\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# 拟合并转换数据\n",
    "scaled_data = scaler.fit_transform(data_with_outliers)\n",
    "\n",
    "print(\"包含异常点的原始数据：\\n\", data_with_outliers)\n",
    "print(\"\\n标准化后的数据：\\n\", scaled_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "包含缺失值的原始数据：\n",
      " [[ 1.  2. nan]\n",
      " [ 4. nan  6.]\n",
      " [ 7.  8.  9.]\n",
      " [nan 11. 12.]\n",
      " [13. 14. 15.]]\n",
      "\n",
      "缺失值插补后的数据：\n",
      " [[ 1.    2.   10.5 ]\n",
      " [ 4.    8.75  6.  ]\n",
      " [ 7.    8.    9.  ]\n",
      " [ 6.25 11.   12.  ]\n",
      " [13.   14.   15.  ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "\n",
    "# 创建包含缺失值的数据\n",
    "data_with_missing = np.array([\n",
    "    [1.0, 2.0, np.nan],\n",
    "    [4.0, np.nan, 6.0],\n",
    "    [7.0, 8.0, 9.0],\n",
    "    [np.nan, 11.0, 12.0],\n",
    "    [13.0, 14.0, 15.0]\n",
    "])\n",
    "\n",
    "# 创建SimpleImputer对象，使用均值填充缺失值\n",
    "imputer = SimpleImputer(strategy='mean')#strategy参数用于指定填充策略，mean表示使用均值填充，median表示使用中位数填充，most_frequent表示使用最频繁的值填充\n",
    "#根据具体场景选择填充策略，例如：\n",
    "# 如果数据中存在大量缺失值，可以使用均值填充，但可能会引入偏差\n",
    "# 如果数据中存在少量缺失值，可以使用中位数填充，但可能会引入偏差\n",
    "# 如果数据中存在大量重复值，可以使用最频繁的值填充，但可能会引入偏差\n",
    "# 如果数据中存在大量异常值，可以使用中位数填充，但可能会引入偏差\n",
    "\n",
    "# 拟合并转换数据\n",
    "imputed_data = imputer.fit_transform(data_with_missing)\n",
    "\n",
    "print(\"包含缺失值的原始数据：\\n\", data_with_missing)\n",
    "print(\"\\n缺失值插补后的数据：\\n\", imputed_data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
